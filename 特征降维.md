- 特征选择的嵌入式、过滤式以及包裹氏三种方式 应用VarianceThreshold实现删除低方差特征
- 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程  降低随机变量的个数
- 两种降维方式  特征选择 主成分分析也是种特征提取
	- 特征选择实在冗余或者无关变量，都是从特征中找出主要特征
		- 过滤式 filter   探究特征本身特点  特征与特征和目标值之间关联  然后筛选出一些来  不用他  
			- 低方差特征来过滤 
			- sklearn.feature_selection.VarianceThreshold(threshold = 0.0)
				- 删除所有低方差特征
				- Variance.fit_transform(X)
                - X:numpy array格式的数据[n_samples,n_features]
				- 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。
			- 相关系数  比如皮尔逊相关系数 就是看相关性
				- 当r>0时，表示两变量正相关，r<0时，两变量为负相关
				- 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系
  				- 当0<|r|<1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱
				- 一般可按三级划分：|r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关
				-  API from scipy.stats import pearsonr
				- x : (N,) array_like
				- y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)
		- 嵌入式  embedded 算法自动选择特征 就算高级了  看看特征和目标值之间的关联
			- 决策树 信息熵  信息增益
			- 正则化：L1  L2
			- 深度学习：卷积等.... 
	- 主成分分析 PCA  
	- 高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量
	- 是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息  一般都是在回归分析和聚类分析中使用
		- sklearn.decomposition.PCA(n_components=None)
		- 将数据分解为较低维数空间
		- n_components:
			- 小数：表示保留百分之多少的信息
			- 整数：减少到多少特征
		- PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]
		- 返回值：转换后指定维度的array





```python
def variance_demo():
    """
    删除低方差特征——特征选择
    :return: None
    """
    data = pd.read_csv("factor_returns.csv")
    print(data)
    # 1、实例化一个转换器类
    transfer = VarianceThreshold(threshold=1)
    # 2、调用fit_transform
    data = transfer.fit_transform(data.iloc[:, 1:10]) #后面的指定数值 所有的行  (1,10]的区间的值
    print("删除低方差特征的结果：\n", data)
    print("形状：\n", data.shape)
 
    return None

```

```python
import pandas as pd
from scipy.stats import pearsonr
 
def pearsonr_demo():
    """
    相关系数计算
    :return: None
    """
    data = pd.read_csv("factor_returns.csv")
 
    factor = ['pe_ratio', 'pb_ratio', 'market_cap', 'return_on_asset_net_profit', 'du_return_on_equity', 'ev',
              'earnings_per_share', 'revenue', 'total_expense']
 
    for i in range(len(factor)):
        for j in range(i, len(factor) - 1):
            print(
                "指标%s与指标%s之间的相关性大小为%f" % (factor[i], factor[j + 1], pearsonr(data[factor[i]], data[factor[j + 1]])[0]))
 
    return None
```

```python
from sklearn.decomposition import PCA

def pca():
    data = [[2,8,4,5], [6,3,0,8], [5,4,9,1]]

    shiliihua = PCA(n_components=0.95)
    data1 = shiliihua.fit_transform(data)
    print(data1)
    print("----------------------")
    shiliihua2 = PCA(n_components=3)
    data2 = shiliihua2.fit_transform(data)
    print(data2)

    return None
pca()
```



