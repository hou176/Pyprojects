- 逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归。由于算法的简单和高效，在实际中应用非常广泛。逻辑回归就是解决二分类问题的利器
- 逻辑回归的输入就是一个线性回归的结果。
- 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例),另外的一个类别会标记为0(反例)。（方便损失计算） 逻辑回归的阈值是可以进行改变的
- 逻辑回归的损失，称之为对数似然损失  
- 优化也是使用梯度下降
	- 使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于1类别的概率，降低原本是0类别的概率。
- sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)
	- solver可选参数:{'liblinear', 'sag', 'saga','newton-cg', 'lbfgs'}，
		- 默认: 'liblinear'；用于优化问题的算法。
		- 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。
		- 对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。
	- penalty：正则化的种类
	- C：正则化力度
- ！我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。


- 分类评估的方法
	- 混淆举证  在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)
- 精确率：预测结果为正例样本中真实为正例的比例（了解）
- 召回率：真实为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力）
- F1-score，反映了模型的稳健型

- 分类评估报告api
- sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )
	- y_true：真实目标值
	- y_pred：估计器预测目标值
	- labels:指定类别对应的数字
	- target_names：目标类别名称
	- return：每个类别精确率与召回率
- 假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题