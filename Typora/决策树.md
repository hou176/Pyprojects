- 决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树。
- 物理学上 熵是混乱程度的代表   系统越有序熵值越高
- 从信息的完整性上进行的描述:当系统的有序状态一致时，数据越集中的地方熵值越小，数据越分散的地方熵值越大。
- 从信息的有序性上进行的描述:当数据量一致时，系统越有序，熵值越低；系统越混乱或者分散，熵值越高。 这个就很像物理熵值
- 信息增益   某特征划分数据集前后的熵的差值  上可以表示样本集合的不确定性  熵值越大   不确定性越大   因此可以用熵得差值来衡量使用当前特征对这个集合划分效果的好坏
-  信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的 C4.5 决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用"增益率" (gain ratio) 来选择最优划分属性.

- class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)
	- criterion
		- 特征选择标准
		- "gini"或者"entropy"，前者代表基尼系数，后者代表信息增益。一默认"gini"，即CART算法。
	- min_samples_split
		- 内部节点再划分所需最小样本数
	- min_samples_leaf
		- 叶子节点最少样本数
	- max_depth
		- 决策树最大深度
		决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间
	- random_state
		- 随机数种子

- 欠拟合就是拟合的不好  数据点有点分散   根本看不出来怎么他的痕迹  过拟合就是很好的和点相符合  曲线同时也受噪音影响  曲线来回波动




```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import  train_test_split
from sklearn.tree import  DecisionTreeClassifier , export_graphviz


data = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")
x = data[["pclass", "age", "sex"]]
y = data["survived"]

x["age"].fillna(x["age"].mean(), inplace=True)

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)

transfer = DictVectorizer(sparse=False)



# 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)
# x.to_dict(orient="records") 需要将数组特征转换成字典数据
x_train = transfer.fit_transform(x_train.to_dict(orient="records"))
x_test = transfer.fit_transform(x_test.to_dict(orient="records"))

# 4.机器学习(决策树)
estimator = DecisionTreeClassifier(criterion="entropy", max_depth=5)
estimator.fit(x_train, y_train)

estimator.score(x_test, y_test)

estimator.predict(x_test)
```
