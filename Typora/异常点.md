# 异常点检测

###数据的异常
- 数据中有异常  也就是噪音   某些情况也是要异常的  如果你是要就异常肯定就不能去掉了
-离群点跟噪声数据不一样，噪声是被观测变量的随机误差或方差。一般而言，噪声在数据分析（包括离群点分析）中不是令人感兴趣的，需要在数据预处理中剔除的，减少对后续模型预估的影响，增加精度。
- 噪音产生原因
	- 业务问题
	- 数据采集时候出现的问题
	- 数据同步问题
	- 对利群点进行挖掘分析之前  需要从中分析利群数据  将垃圾数据去掉
- 常用的异常点检测
	- 基于统计的异常检测方法 泊松分布  正态分布的分布规律找到异常
	- 基于距离的异常  KNN 聚类  中的K均值
	- 基于密度的利群检测方法
	- 基于偏差的异常点检测方法
	- 时间序列 
- 在大多数场景下，通过非监督式方法实现的异常检测的结果只能用来缩小排查范围、为业务的执行提供更加精准和高效的执行目标而已


### sklearn常用集中操作
- sklearn 中检测异常
	- one-class SVM（svm.OneClassSVM）：该方法是SVM的变体，基于libsvm实现。它对数据集的分布没有假设，可应用到非高斯分布的数据，对高维数据非常有效。也是类似二分类问题，不是A就是B，这种情况可以在一定平面内找出异常事件   不能是正态分布  而且一般都是做新颖点检测   而且one——class训练很怕噪音   新颖点也叫奇异点 就是判断待测样本到底是不是在原来数据的概率分布内。概率学上认为，所有的数据都有它的隐藏的分布模式，这种分布模式可以由概率模型来具象化。

	- EllipticEnvelope（covariance.EllipticEnvelope）：这是基于协方差的稳健估计，可用于异常检测，但前提是数据是高斯分布的。 

	- Isolation Forest（sklearn.ensemble.IsolationForest）：一种与随机森林类似，都是高效的集成算法，算法鲁棒性高且对数据集的分布无假设。另外，基于树的集成算法，对数据特征的要求宽松。孤立森林其实就是二叉树的方式一致延伸分类   如果是不是异常值的话肯定差别不大就是分好几层  树的深度很深   异常值就不是了  树的深度很浅就出来了    鲁棒的意思是足够稳定，绝不过拟合

	- LocalOutlierFactor（sklearn.neighbors.LocalOutlierFactor）：基于近邻的密度来估计局部偏差，然后确定异常因子，属于KNN的变体。

- 总结
	- 如果训练集很干净：使用one-class svm
	- 训练集不干净，如果能保证训练集能基本覆盖正常样本：LOF
	- 训练集不干净，如果能保证训练集服从正太分布：EllipticEnvelope
	- 训练集不干净，不能保证训练集基本覆盖正常样本：isolation forest

### 正态分布
- 正太分布其实经常见的到  两边少  中间多   有一个中间属性只很多 
- 实际有很多的分布  比如线性的   曲线的   但是如果我们吧大量的不同分布的随机变量加起来  变量都将会具有正态分布
- 正态分布只依赖数据集的两个特征  样本的均值和方差
- 正态分布是许多随机性很高的变量相加得到的总和   大自然有很多的正态分布

