- 线性回归常用的地方   贷款预测  销售预测  房价的预测
- 线性回归就跟函数差不多  原理是这样的  但是细节确实按照情况来走  是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
- 房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率
	- 这就算是一个关系  而特征值和目标值的之间的关系  可以理解为线性模型
- 线性回归中主要有两种模型  线性关系  非线性关系
- 线性  API
	-  sklearn.linear_model.LinearRegression()
		- LinearRegression.coef_：回归系数
- 步骤分析  获取数据  数据基本处理  特征工程  机器学习  模型评估
- 线性回归的优化
	- 正规方程   需要推导的方程
	- 梯度下降   比如从山上下来  就一段一段的选择路程  一直选择一个小路程的最低点  然后从最低点慢慢在往下走  肯定会到最低谷的
	- 梯度下降算法  随机梯度下降算法  小批量梯度下降算法  随机平均梯度下降算法
- sklearn.linear_model.LinearRegression(fit_intercept=True)
	通过正规方程优化
	- 参数
		- fit_intercept：是否计算偏置
	- 属性
		- LinearRegression.coef_：回归系数
		- LinearRegression.intercept_：偏置
sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)
	- SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。
	- 参数：
		- loss:损失类型
			- loss=”squared_loss”: 普通最小二乘法
		- fit_intercept：是否计算偏置
		- learning_rate : string, optional
			- 学习率填充
			- 'constant': eta = eta0
			- 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
			- 'invscaling': eta = eta0 / pow(t, power_t)
				- power_t=0.25:存在父类当中
		- 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。
	- 属性：
		SGDRegressor.coef_：回归系数
		SGDRegressor.intercept_：偏置

- 过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)
	- 从新清洗数据
	- 增大数据的训练量
	- 正则化
		- 数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化
		- 正则化有L1正则化  L2正则化
	- 减少特征为维度 防止维灾难 
- 欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)
	- 添加其他特征
	- 添加多项式特征

- 非线性关系的数据，也就是存在很多无用的特征或者现实中的事物特征跟目标值的关系并不是简单的线性关系。

- 岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）以达到在拟合数据的同时，使模型权重尽可能小的目的
- sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
	- 具有l2正则化的线性回归
	- alpha:正则化力度，也叫 λ
		- λ取值：0~1 1~10
	- solver:会根据数据自动选择优化方法
		- sag:如果数据集、特征都比较大，选择该随机梯度下降优化 
	- normalize:数据是否进行标准化
		- normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
	- Ridge.coef_:回归权重
	- Ridge.intercept_:回归偏置
- Ridge方法相当于SGDRegressor(penalty='l2', loss="squared_loss"),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)



