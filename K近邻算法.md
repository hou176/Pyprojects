- k近邻算法就是用你的邻居来判断你怎么样  有一个距离公式
- API  sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
	- n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
	- algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)

- k近邻算法用来分为测试集和训练的    训练时候  因为是预测看准不准   结果就在那里摆着呢   拿训练集训练  就是得到他们关系   然后在用第一测试  按照训练出来的关系   x_test经过关系   看看和y_test有多少准确度  就是为了这个   在过程中  注意KNN的选择近邻的数量   还来注意分数据集时候种子数量不同   他们效果就不同  还有要标准化
- k值取很小：容易受到异常点的影响   k值取很大：受到样本均衡的问题
- K-近邻总结
	- 优点：
		- 简单，易于理解，易于实现，无需训练
	- 缺点：
		- 懒惰算法，对测试样本分类时的计算量大，内存开销大
		- 必须指定K值，K值选择不当则分类精度不能保证
	- 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试


- 优点：
	- 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
	- 对缺失数据不太敏感，算法也比较简单，常用于文本分类。
	- 分类准确度高，速度快
缺点：
	- 由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好