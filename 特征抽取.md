- 特征工程专业的技术处理数据  直接能影响机器学习  就是比较厉害一点的数据处理
- 特征抽取  数据预处理  特征降维

# 特征抽取

- 将任意的数据  比如文本  图片 转换成机器学习的数字特征  就是为了计算机更好的处理数据
	- 字段特征提取 特征离散化
	- 文本特征
	- 图像的特征

- API   sklearn.feature_extraction
- sklearn.feature_extraction.DictVectorizer(sparse=True,…)
	- DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵
	- DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式
	- DictVectorizer.get_feature_names() 返回类别名称
- 使用实例化类 DictVectorizer 调用fit_transform方法输入数据并转换（注意返回格式）
- 使用onehot的编码方式  使用它实例化  然后出来一堆数字了  特征当中存在类别信息我们都会做onehot的编码处理  其实就是一个变相的保存方法 毕竟都说是编码方式了   就是以数字的方式记住其他类型文件   比如二进制记住所有的文件是一个道理
```python
from sklearn import feature_extraction
# 特种工程的API
from sklearn.feature_extraction import  DictVectorizer


# # 对字典一个特征的提取
# def dict_demo():
#     data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
#     beishilihua = DictVectorizer(sparse=False)
#     data = beishilihua.fit_transform(data)
#     print(data)
#     print("name:",beishilihua.get_feature_names())
#
#     return None
#
# dict_demo()

def dict_demo2():
    data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
    beishilihua = DictVectorizer(sparse=True)
    data = beishilihua.fit_transform(data)
    # data = beishilihua.inverse_transform(data)
    # print(data2)
    print(data)
    print("name:",beishilihua.get_feature_names())
    # print(data1.get_feature_names())
    return None

dict_demo2()


```
- 对文本进行特征值话用的识别的API
- API   
from sklearn.feature_extraction.text import CountVectorizer
    - sklearn.feature_extraction.text.CountVectorizer(stop_words=[]) 返回词频矩阵
    - CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵
    - CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格
    - CountVectorizer.get_feature_names() 返回值:单词列表
    - sklearn.feature_extraction.text.TfidfVectorizer
- 中文必须要分词的效果   在jieba上用
```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba


def cut_word(text):
    """
    对中文进行分词
    "我爱北京天安门"————>"我 爱 北京 天安门"
    :param text:
    :return: text
    """
    # 用结巴对中文字符串进行分词
    text = " ".join(list(jieba.cut(text)))

    return text


def text_chinese_count_demo2():
    """
    对中文进行特征抽取
    :return: None
    """
    data = ["你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 将原始数据转换成分好词的形式
    text_list = []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(text_list)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
text_chinese_count_demo2()

```

- TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
- 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到
